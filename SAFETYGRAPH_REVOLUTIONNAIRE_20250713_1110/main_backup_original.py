"""
SafeGraph - Prototype Principal (Version Claude API)
D√©monstration du syst√®me multi-agent d'analyse de culture s√©curit√©
"""

import asyncio
from datetime import datetime
from src.core.graph import create_safety_graph
from src.core.state import create_initial_state
from src.core.config import config

def display_welcome():
    """Affiche le message de bienvenue avec info API"""
    print("=" * 60)
    print("üîí SAFEGRAPH - Prototype Culture S√©curit√©")
    print("=" * 60)
    print("Syst√®me multi-agent d'analyse de culture s√©curit√©")
    print("Bas√© sur LangGraph et sp√©cialis√© par secteur SCIAN")
    print("=" * 60)
    
    # Afficher le LLM configur√©
    llm_status = f"ü§ñ LLM configur√© : {config.preferred_llm.upper()}"
    if config.preferred_llm == "claude":
        llm_status += f" ({config.claude_model})"
    elif config.preferred_llm == "openai":
        llm_status += f" ({config.openai_model})"
    else:
        llm_status += " (Mode simulation - aucune API)"
    
    print(llm_status)
    print("=" * 60)
    print()

def display_results(final_state):
    """Affiche les r√©sultats de l'analyse"""
    print("\n" + "=" * 60)
    print("üìä R√âSULTATS DE L'ANALYSE")
    print("=" * 60)
    
    # Trace des agents
    print("\nüîÑ Agents ex√©cut√©s :")
    for agent in final_state.get("agent_trace", []):
        print(f"  ‚úÖ {agent}")
    
    # LLM utilis√©
    context = final_state.get("context", {})
    llm_used = context.get("llm_used", config.preferred_llm)
    print(f"\nü§ñ LLM utilis√© : {llm_used.upper()}")
    
    # Intent d√©tect√©
    intent = final_state.get("intent")
    print(f"\nüéØ Intention d√©tect√©e : {intent.value if intent else 'Non d√©termin√©e'}")
    
    # Secteur SCIAN
    sector = final_state.get("scian_sector")
    sector_name = config.scian_sectors.get(sector, "Non sp√©cifi√©") if sector else "Non sp√©cifi√©"
    print(f"üè≠ Secteur SCIAN : {sector} - {sector_name}")
    
    # Confiance de la classification
    confidence = context.get("confidence_score", 0)
    print(f"üìà Confiance classification : {confidence:.1%}")
    
    # Donn√©es collect√©es
    collection_results = final_state.get("collection_results", [])
    if collection_results:
        latest = collection_results[-1]
        print(f"\nüìã Donn√©es collect√©es :")
        print(f"  ‚Ä¢ Questionnaire : {latest.get('questionnaire_id', 'N/A')}")
        print(f"  ‚Ä¢ Taux de compl√©tion : {latest.get('completion_rate', 0):.1%}")
        
        preliminary_scores = latest.get('preliminary_scores', {})
        if preliminary_scores and 'global' in preliminary_scores:
            global_score = preliminary_scores['global'].get('raw_score', 'N/A')
            print(f"  ‚Ä¢ Score global pr√©liminaire : {global_score}")
    
    # Analyse des risques
    analysis = final_state.get("analysis", {})
    if analysis:
        risk_classification = analysis.get("risk_classification", "Non d√©termin√©")
        print(f"\n‚ö†Ô∏è  Classification de risque : {risk_classification}")
        
        # Qualit√© des donn√©es
        data_quality = analysis.get("data_quality", 0)
        print(f"üìä Qualit√© des donn√©es : {data_quality:.1%}")
        
        key_findings = analysis.get("key_findings", [])
        if key_findings:
            print("\nüîç Conclusions cl√©s :")
            for finding in key_findings:
                print(f"  ‚Ä¢ {finding}")
    
    # Recommandations
    recommendations = final_state.get("recommendations", [])
    if recommendations:
        print(f"\nüí° Recommandations g√©n√©r√©es : {len(recommendations)}")
        
        # Compter par priorit√©
        priority_counts = {}
        for rec in recommendations:
            priority = rec.get("priority", "Medium")
            priority_counts[priority] = priority_counts.get(priority, 0) + 1
        
        for priority, count in priority_counts.items():
            print(f"  ‚Ä¢ {priority} : {count}")
        
        # Afficher les 3 premi√®res recommandations
        print("\nüéØ Top 3 recommandations :")
        for i, rec in enumerate(recommendations[:3]):
            priority = rec.get("priority", "Medium")
            title = rec.get("title", "Recommandation")
            source = rec.get("source", "unknown")
            print(f"  {i+1}. [{priority}] {title} ({source})")
    
    # Plan d'action
    action_plan = final_state.get("action_plan", {})
    if action_plan:
        summary = action_plan.get("summary", {})
        total_effort = summary.get("estimated_total_effort", "N/A")
        print(f"\nüìã Plan d'action : {total_effort} effort estim√©")
        
        # Timeline
        timeline = action_plan.get("timeline", {})
        critical_actions = timeline.get("critical_actions", [])
        if critical_actions:
            print(f"üö® Actions critiques : {len(critical_actions)}")
    
    # Actions prioritaires
    priority_actions = final_state.get("priority_actions", [])
    if priority_actions:
        print(f"\nüö® Actions prioritaires imm√©diates :")
        for action in priority_actions:
            print(f"  ‚Ä¢ {action}")
    
    # Erreurs √©ventuelles
    errors = final_state.get("errors", [])
    if errors:
        print(f"\n‚ùå Erreurs d√©tect√©es :")
        for error in errors:
            print(f"  ‚Ä¢ {error}")
    
    print("\n" + "=" * 60)

async def run_demo():
    """Ex√©cute une d√©monstration du prototype"""
    
    display_welcome()
    
    # Exemples de requ√™tes pour la d√©mo
    demo_queries = [
        {
            "query": "Je travaille en construction et j'aimerais √©valuer ma culture s√©curit√© sur le chantier",
            "description": "√âvaluation Construction"
        },
        {
            "query": "Analyse des risques pour un conducteur de transport longue distance avec fatigue", 
            "description": "Analyse Transport"
        },
        {
            "query": "Recommandations pour am√©liorer la s√©curit√© dans notre service de soins d'urgence",
            "description": "Recommandations Sant√©"
        },
        {
            "query": "Suivi des progr√®s s√©curit√© apr√®s formation du personnel de maintenance",
            "description": "Monitoring Maintenance"
        }
    ]
    
    print("Requ√™tes de d√©monstration disponibles :")
    for i, demo in enumerate(demo_queries):
        print(f"  {i+1}. {demo['description']}")
    
    # S√©lection interactive ou automatique
    try:
        choice = input("\nChoisissez une requ√™te (1-4) ou Entr√©e pour toutes : ")
        if choice.isdigit() and 1 <= int(choice) <= 4:
            selected_demos = [demo_queries[int(choice)-1]]
        else:
            selected_demos = demo_queries
    except:
        selected_demos = demo_queries
    
    # Cr√©er le graphe SafeGraph
    print("\nüîß Initialisation du graphe SafeGraph...")
    try:
        graph = create_safety_graph()
        print("‚úÖ Graphe cr√©√© avec succ√®s")
    except Exception as e:
        print(f"‚ùå Erreur cr√©ation graphe : {e}")
        return
    
    # Ex√©cuter chaque d√©mo
    for demo in selected_demos:
        print(f"\n{'='*60}")
        print(f"üß™ D√âMONSTRATION : {demo['description']}")
        print(f"Query : {demo['query']}")
        print(f"{'='*60}")
        
        try:
            # Cr√©er √©tat initial
            initial_state = create_initial_state(demo['query'])
            
            # Ex√©cuter le graphe
            print("\n‚öôÔ∏è  Ex√©cution du workflow SafeGraph...")
            final_state = await graph.ainvoke(initial_state)
            
            # Afficher les r√©sultats
            display_results(final_state)
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'ex√©cution : {e}")
            print(f"D√©tails : {type(e).__name__}")
            import traceback
            if config.debug_mode:
                traceback.print_exc()
        
        # Pause entre les d√©mos
        if len(selected_demos) > 1:
            input("\nAppuyez sur Entr√©e pour continuer...")

def run_sync_demo():
    """Version synchrone pour compatibilit√©"""
    
    display_welcome()
    
    print("üîß Mode d√©mo synchrone (simulation)...")
    
    # Test de configuration API
    if config.has_claude_api:
        print("‚úÖ API Claude configur√©e")
    elif config.has_openai_api:
        print("‚úÖ API OpenAI configur√©e")
    else:
        print("‚ö†Ô∏è  Aucune API configur√©e - Mode simulation")
    
    # Simulation simple sans LangGraph complet
    test_queries = [
        "Je travaille en construction et j'aimerais √©valuer ma culture s√©curit√©",
        "Analyse des risques pour conducteur de transport",
        "Recommandations pour am√©liorer la s√©curit√© des soins"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nüìù Test {i} avec requ√™te : {query}")
        
        # Cr√©er √©tat de test
        test_state = create_initial_state(query)
        
        print(f"  ‚úÖ √âtat initial cr√©√©")
        print(f"    ‚Ä¢ Session ID : {test_state['session_id']}")
        print(f"    ‚Ä¢ LLM pr√©f√©r√© : {config.preferred_llm}")
    
    print(f"\nüèóÔ∏è  Structure du projet SafeGraph initialis√©e avec succ√®s !")
    print(f"  ‚Ä¢ {len(config.scian_sectors)} secteurs SCIAN support√©s")
    print(f"  ‚Ä¢ Agents multi-sp√©cialis√©s op√©rationnels")
    print(f"  ‚Ä¢ Architecture LangGraph pr√™te")
    print(f"  ‚Ä¢ Support Claude + OpenAI")
    
    print(f"\nüöÄ Prototype SafeGraph pr√™t pour d√©veloppement complet !")

if __name__ == "__main__":
    print("Lancement du prototype SafeGraph...")
    
    # V√©rifier configuration
    if config.preferred_llm == "none":
        print("‚ö†Ô∏è  Aucune API LLM configur√©e - Mode simulation activ√©")
        run_sync_demo()
    else:
        print(f"üîë API {config.preferred_llm.upper()} d√©tect√©e - Mode complet")
        try:
            asyncio.run(run_demo())
        except Exception as e:
            print(f"Erreur mode async : {e}")
            print("Basculement en mode simulation...")
            run_sync_demo()