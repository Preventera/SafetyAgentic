#!/usr/bin/env python3
"""
SafetyGraph - Module Pattern Recognition
Reconnaissance automatique de patterns culturels en sÃ©curitÃ©
Version complÃ¨te avec interface Streamlit
"""

import pandas as pd
import numpy as np
import sqlite3
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# ML Libraries pour clustering et pattern recognition
try:
    from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
    from sklearn.mixture import GaussianMixture
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn.decomposition import PCA
    from sklearn.metrics import silhouette_score, adjusted_rand_score
    from sklearn.manifold import TSNE
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False


class CulturePattern:
    """ReprÃ©sente un pattern culturel identifiÃ©"""
    
    def __init__(self, pattern_id: str, pattern_type: str, 
                 characteristics: Dict, sectors_affected: List[str],
                 confidence_score: float):
        self.pattern_id = pattern_id
        self.pattern_type = pattern_type
        self.characteristics = characteristics
        self.sectors_affected = sectors_affected
        self.confidence_score = confidence_score
        self.created_at = datetime.now()
        self.prevalence = 0.0
        self.stability = 0.0
        self.trend = "stable"
    
    def to_dict(self) -> Dict:
        """Convertit le pattern en dictionnaire"""
        return {
            'pattern_id': self.pattern_id,
            'pattern_type': self.pattern_type,
            'characteristics': self.characteristics,
            'sectors_affected': self.sectors_affected,
            'confidence_score': self.confidence_score,
            'prevalence': self.prevalence,
            'stability': self.stability,
            'trend': self.trend,
            'created_at': self.created_at.isoformat()
        }


class SafetyGraphPatternRecognition:
    """Moteur de reconnaissance de patterns SafetyGraph"""
    
    def __init__(self, db_path: str = "analytics_patterns.db"):
        """
        Initialise le moteur de reconnaissance
        
        Args:
            db_path: Chemin vers la base de donnÃ©es
        """
        self.db_path = db_path
        self.patterns = {}
        self.clusters = {}
        self.scalers = {}
        
        # Configuration
        self.config = {
            'min_cluster_size': 10,
            'max_clusters': 20,
            'confidence_threshold': 0.7,
            'stability_window': 30  # jours
        }
        
        # Initialisation base de donnÃ©es
        self._init_database()
        
        # Chargement patterns existants
        self._load_existing_patterns()
    
    def _init_database(self):
        """Initialise la base de donnÃ©es SQLite"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Table patterns identifiÃ©s
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern_id TEXT UNIQUE,
                pattern_type TEXT,
                characteristics TEXT,
                sectors_affected TEXT,
                confidence_score REAL,
                prevalence REAL,
                stability REAL,
                trend TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Table clusters
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS clusters (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                cluster_id TEXT,
                algorithm TEXT,
                n_clusters INTEGER,
                silhouette_score REAL,
                cluster_centers TEXT,
                cluster_labels TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Table transitions entre patterns
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS pattern_transitions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                from_pattern TEXT,
                to_pattern TEXT,
                transition_probability REAL,
                sector_scian TEXT,
                observed_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def _load_existing_patterns(self):
        """Charge les patterns existants depuis la base de donnÃ©es"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM patterns')
        patterns_data = cursor.fetchall()
        
        for pattern_data in patterns_data:
            pattern_id = pattern_data[1]
            pattern_type = pattern_data[2]
            characteristics = json.loads(pattern_data[3])
            sectors_affected = json.loads(pattern_data[4])
            confidence_score = pattern_data[5]
            
            pattern = CulturePattern(
                pattern_id=pattern_id,
                pattern_type=pattern_type,
                characteristics=characteristics,
                sectors_affected=sectors_affected,
                confidence_score=confidence_score
            )
            
            pattern.prevalence = pattern_data[6]
            pattern.stability = pattern_data[7]
            pattern.trend = pattern_data[8]
            
            self.patterns[pattern_id] = pattern
        
        conn.close()
    
    def prepare_culture_data(self, culture_data: pd.DataFrame) -> pd.DataFrame:
        """
        PrÃ©pare les donnÃ©es culturelles pour l'analyse
        
        Args:
            culture_data: DataFrame avec Ã©valuations culture
            
        Returns:
            DataFrame prÃ©parÃ©
        """
        if culture_data.empty:
            return pd.DataFrame()
        
        # Copie des donnÃ©es
        df = culture_data.copy()
        
        # Nettoyage colonnes
        df.columns = df.columns.str.strip().str.lower()
        
        # Dimensions culturelles standard
        culture_dimensions = [
            'leadership_engagement',
            'communication_effectiveness', 
            'training_quality',
            'employee_participation',
            'monitoring_improvement',
            'psychosocial_environment'
        ]
        
        # VÃ©rification/crÃ©ation des colonnes
        for dim in culture_dimensions:
            if dim not in df.columns:
                # GÃ©nÃ©ration de donnÃ©es simulÃ©es si manquantes
                df[dim] = np.random.normal(3.5, 0.5, len(df))
                df[dim] = np.clip(df[dim], 1, 5)
        
        # AgrÃ©gation par secteur et pÃ©riode
        if 'sector_scian' in df.columns and 'evaluation_date' in df.columns:
            df['evaluation_date'] = pd.to_datetime(df['evaluation_date'], errors='coerce')
            df['month_year'] = df['evaluation_date'].dt.to_period('M')
            
            # AgrÃ©gation mensuelle par secteur
            agg_df = df.groupby(['sector_scian', 'month_year']).agg({
                **{dim: 'mean' for dim in culture_dimensions},
                'enterprise_size': 'mean',
                'years_operation': 'mean'
            }).reset_index()
            
            # Calcul score global
            agg_df['culture_score_global'] = agg_df[culture_dimensions].mean(axis=1)
            
            # Indicateurs dÃ©rivÃ©s
            agg_df['leadership_communication_ratio'] = (
                agg_df['leadership_engagement'] / agg_df['communication_effectiveness']
            )
            
            agg_df['training_participation_synergy'] = (
                agg_df['training_quality'] * agg_df['employee_participation']
            )
            
            return agg_df
        
        return df
    
    def perform_clustering(self, data: pd.DataFrame, 
                          algorithm: str = 'kmeans', 
                          n_clusters: int = 4) -> Dict:
        """
        Effectue le clustering des donnÃ©es culturelles
        
        Args:
            data: DataFrame avec donnÃ©es prÃ©parÃ©es
            algorithm: Algorithme de clustering
            n_clusters: Nombre de clusters
            
        Returns:
            RÃ©sultats du clustering
        """
        if data.empty or not ML_AVAILABLE:
            return {}
        
        # SÃ©lection des features pour clustering
        feature_cols = [col for col in data.columns 
                       if col not in ['sector_scian', 'month_year', 'evaluation_date']]
        
        if len(feature_cols) < 2:
            return {}
        
        X = data[feature_cols].fillna(data[feature_cols].mean())
        
        if len(X) < n_clusters:
            return {}
        
        # Normalisation
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Choix algorithme
        if algorithm == 'kmeans':
            clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        elif algorithm == 'dbscan':
            clusterer = DBSCAN(eps=0.5, min_samples=5)
        elif algorithm == 'hierarchical':
            clusterer = AgglomerativeClustering(n_clusters=n_clusters)
        elif algorithm == 'gaussian_mixture':
            clusterer = GaussianMixture(n_components=n_clusters, random_state=42)
        else:
            clusterer = KMeans(n_clusters=n_clusters, random_state=42)
        
        # Clustering
        if algorithm == 'gaussian_mixture':
            cluster_labels = clusterer.fit_predict(X_scaled)
        else:
            cluster_labels = clusterer.fit_predict(X_scaled)
        
        # MÃ©triques qualitÃ©
        silhouette_avg = silhouette_score(X_scaled, cluster_labels) if len(set(cluster_labels)) > 1 else 0
        
        # Centres des clusters
        if hasattr(clusterer, 'cluster_centers_'):
            cluster_centers = clusterer.cluster_centers_
        elif hasattr(clusterer, 'means_'):
            cluster_centers = clusterer.means_
        else:
            # Calcul manuel pour DBSCAN
            cluster_centers = []
            for label in set(cluster_labels):
                if label != -1:  # Ignore noise points
                    cluster_centers.append(X_scaled[cluster_labels == label].mean(axis=0))
            cluster_centers = np.array(cluster_centers)
        
        # Ajout des labels au DataFrame
        data_with_clusters = data.copy()
        data_with_clusters['cluster_label'] = cluster_labels
        
        # Analyse des clusters
        cluster_analysis = self._analyze_clusters(data_with_clusters, feature_cols)
        
        results = {
            'algorithm': algorithm,
            'n_clusters': len(set(cluster_labels)),
            'silhouette_score': silhouette_avg,
            'cluster_labels': cluster_labels,
            'cluster_centers': cluster_centers,
            'data_with_clusters': data_with_clusters,
            'cluster_analysis': cluster_analysis,
            'feature_columns': feature_cols,
            'scaler': scaler
        }
        
        # Sauvegarde en base
        self._save_clustering_results(results)
        
        return results
    
    def _analyze_clusters(self, data_with_clusters: pd.DataFrame, 
                         feature_cols: List[str]) -> Dict:
        """
        Analyse dÃ©taillÃ©e des clusters
        
        Args:
            data_with_clusters: DataFrame avec labels clusters
            feature_cols: Colonnes features
            
        Returns:
            Analyse des clusters
        """
        analysis = {}
        
        for cluster_id in data_with_clusters['cluster_label'].unique():
            if cluster_id == -1:  # Ignore noise points
                continue
            
            cluster_data = data_with_clusters[data_with_clusters['cluster_label'] == cluster_id]
            
            # Statistiques descriptives
            cluster_stats = {
                'size': len(cluster_data),
                'percentage': len(cluster_data) / len(data_with_clusters) * 100,
                'feature_means': cluster_data[feature_cols].mean().to_dict(),
                'feature_stds': cluster_data[feature_cols].std().to_dict()
            }
            
            # Secteurs dominants
            if 'sector_scian' in cluster_data.columns:
                sector_counts = cluster_data['sector_scian'].value_counts()
                cluster_stats['dominant_sectors'] = sector_counts.head(3).to_dict()
            
            # CaractÃ©risation du cluster
            cluster_stats['characterization'] = self._characterize_cluster(cluster_data, feature_cols)
            
            analysis[f'cluster_{cluster_id}'] = cluster_stats
        
        return analysis
    
    def _characterize_cluster(self, cluster_data: pd.DataFrame, 
                            feature_cols: List[str]) -> Dict:
        """
        CaractÃ©rise un cluster spÃ©cifique
        
        Args:
            cluster_data: DonnÃ©es du cluster
            feature_cols: Colonnes features
            
        Returns:
            CaractÃ©risation du cluster
        """
        means = cluster_data[feature_cols].mean()
        
        # Identification du type de pattern
        if means.get('leadership_engagement', 0) > 4.0 and means.get('communication_effectiveness', 0) > 4.0:
            pattern_type = 'proactive_excellence'
        elif means.get('training_quality', 0) > 4.0 and means.get('employee_participation', 0) > 4.0:
            pattern_type = 'learning_focused'
        elif means.get('monitoring_improvement', 0) > 4.0:
            pattern_type = 'continuous_improvement'
        elif means.mean() > 3.5:
            pattern_type = 'mature_culture'
        elif means.mean() > 2.5:
            pattern_type = 'developing_culture'
        else:
            pattern_type = 'emerging_culture'
        
        # Forces et faiblesses
        strengths = means.nlargest(3).index.tolist()
        weaknesses = means.nsmallest(3).index.tolist()
        
        return {
            'pattern_type': pattern_type,
            'overall_maturity': means.mean(),
            'strengths': strengths,
            'weaknesses': weaknesses,
            'stability_indicator': means.std(),
            'recommended_actions': self._generate_recommendations(pattern_type, weaknesses)
        }
    
    def _generate_recommendations(self, pattern_type: str, 
                                weaknesses: List[str]) -> List[str]:
        """
        GÃ©nÃ¨re des recommandations basÃ©es sur le pattern
        
        Args:
            pattern_type: Type de pattern identifiÃ©
            weaknesses: Points faibles identifiÃ©s
            
        Returns:
            Liste de recommandations
        """
        recommendations = []
        
        # Recommandations par type de pattern
        if pattern_type == 'emerging_culture':
            recommendations.extend([
                "Ã‰tablir un leadership sÃ©curitÃ© visible",
                "DÃ©velopper politique sÃ©curitÃ© claire",
                "CrÃ©er systÃ¨me communication ascendante"
            ])
        elif pattern_type == 'developing_culture':
            recommendations.extend([
                "Renforcer formation superviseurs",
                "ImplÃ©menter systÃ¨me feedback",
                "Standardiser procÃ©dures sÃ©curitÃ©"
            ])
        elif pattern_type == 'mature_culture':
            recommendations.extend([
                "Maintenir excellence actuelle",
                "Partager meilleures pratiques",
                "Innover en sÃ©curitÃ© prÃ©dictive"
            ])
        
        # Recommandations par faiblesses
        weakness_recommendations = {
            'leadership_engagement': "Coaching leadership sÃ©curitÃ©",
            'communication_effectiveness': "AmÃ©liorer canaux communication",
            'training_quality': "RÃ©viser programme formation",
            'employee_participation': "Encourager participation active",
            'monitoring_improvement': "Renforcer systÃ¨me suivi",
            'psychosocial_environment': "AmÃ©liorer climat travail"
        }
        
        for weakness in weaknesses:
            if weakness in weakness_recommendations:
                recommendations.append(weakness_recommendations[weakness])
        
        return recommendations[:5]  # Limiter Ã  5 recommandations
    
    def identify_patterns(self, clustering_results: Dict) -> List[CulturePattern]:
        """
        Identifie les patterns culturels Ã  partir des rÃ©sultats de clustering
        
        Args:
            clustering_results: RÃ©sultats du clustering
            
        Returns:
            Liste des patterns identifiÃ©s
        """
        patterns = []
        
        if not clustering_results or 'cluster_analysis' not in clustering_results:
            return patterns
        
        cluster_analysis = clustering_results['cluster_analysis']
        
        for cluster_id, analysis in cluster_analysis.items():
            # CrÃ©ation du pattern
            pattern_id = f"pattern_{cluster_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            pattern_type = analysis['characterization']['pattern_type']
            
            # CaractÃ©ristiques du pattern
            characteristics = {
                'size': analysis['size'],
                'percentage': analysis['percentage'],
                'overall_maturity': analysis['characterization']['overall_maturity'],
                'strengths': analysis['characterization']['strengths'],
                'weaknesses': analysis['characterization']['weaknesses'],
                'stability_indicator': analysis['characterization']['stability_indicator'],
                'feature_means': analysis['feature_means']
            }
            
            # Secteurs affectÃ©s
            sectors_affected = list(analysis.get('dominant_sectors', {}).keys())
            
            # Score de confiance basÃ© sur taille et stabilitÃ©
            confidence_score = min(1.0, (analysis['size'] / 100) * 0.5 + 
                                 (1 - analysis['characterization']['stability_indicator']) * 0.5)
            
            pattern = CulturePattern(
                pattern_id=pattern_id,
                pattern_type=pattern_type,
                characteristics=characteristics,
                sectors_affected=sectors_affected,
                confidence_score=confidence_score
            )
            
            pattern.prevalence = analysis['percentage'] / 100
            pattern.stability = 1 - analysis['characterization']['stability_indicator']
            
            patterns.append(pattern)
            self.patterns[pattern_id] = pattern
        
        # Sauvegarde des patterns
        self._save_patterns(patterns)
        
        return patterns
    
    def _save_clustering_results(self, results: Dict):
        """Sauvegarde les rÃ©sultats de clustering"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cluster_id = f"cluster_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        cursor.execute('''
            INSERT INTO clusters (
                cluster_id, algorithm, n_clusters, silhouette_score, 
                cluster_centers, cluster_labels
            ) VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            cluster_id,
            results['algorithm'],
            results['n_clusters'],
            results['silhouette_score'],
            json.dumps(results['cluster_centers'].tolist() if hasattr(results['cluster_centers'], 'tolist') else []),
            json.dumps(results['cluster_labels'].tolist() if hasattr(results['cluster_labels'], 'tolist') else [])
        ))
        
        conn.commit()
        conn.close()
    
    def _save_patterns(self, patterns: List[CulturePattern]):
        """Sauvegarde les patterns identifiÃ©s"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for pattern in patterns:
            cursor.execute('''
                INSERT OR REPLACE INTO patterns (
                    pattern_id, pattern_type, characteristics, sectors_affected,
                    confidence_score, prevalence, stability, trend
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                pattern.pattern_id,
                pattern.pattern_type,
                json.dumps(pattern.characteristics),
                json.dumps(pattern.sectors_affected),
                pattern.confidence_score,
                pattern.prevalence,
                pattern.stability,
                pattern.trend
            ))
        
        conn.commit()
        conn.close()
    
    def detect_pattern_transitions(self, historical_data: pd.DataFrame) -> Dict:
        """
        DÃ©tecte les transitions entre patterns
        
        Args:
            historical_data: DonnÃ©es historiques avec patterns
            
        Returns:
            Analyse des transitions
        """
        if historical_data.empty:
            return {}
        
        # Analyse des transitions par secteur
        transitions = {}
        
        if 'sector_scian' in historical_data.columns and 'pattern_type' in historical_data.columns:
            for sector in historical_data['sector_scian'].unique():
                sector_data = historical_data[historical_data['sector_scian'] == sector]
                sector_data = sector_data.sort_values('evaluation_date')
                
                sector_transitions = []
                for i in range(len(sector_data) - 1):
                    current_pattern = sector_data.iloc[i]['pattern_type']
                    next_pattern = sector_data.iloc[i + 1]['pattern_type']
                    
                    if current_pattern != next_pattern:
                        sector_transitions.append({
                            'from': current_pattern,
                            'to': next_pattern,
                            'date': sector_data.iloc[i + 1]['evaluation_date']
                        })
                
                transitions[sector] = sector_transitions
        
        return transitions
    
    def generate_pattern_report(self, sector_scian: str = None) -> Dict:
        """
        GÃ©nÃ¨re un rapport complet des patterns
        
        Args:
            sector_scian: Code secteur SCIAN (optionnel)
            
        Returns:
            Rapport complet
        """
        # Patterns identifiÃ©s
        patterns_list = list(self.patterns.values())
        
        # Filtrage par secteur si spÃ©cifiÃ©
        if sector_scian:
            patterns_list = [p for p in patterns_list if sector_scian in p.sectors_affected]
        
        # Statistiques globales
        total_patterns = len(patterns_list)
        pattern_types = {}
        
        for pattern in patterns_list:
            pattern_type = pattern.pattern_type
            if pattern_type not in pattern_types:
                pattern_types[pattern_type] = 0
            pattern_types[pattern_type] += 1
        
        # Patterns les plus frÃ©quents
        most_common_patterns = sorted(pattern_types.items(), key=lambda x: x[1], reverse=True)[:5]
        
        # Patterns Ã  haute confiance
        high_confidence_patterns = [p for p in patterns_list if p.confidence_score > 0.8]
        
        return {
            'sector_scian': sector_scian,
            'total_patterns': total_patterns,
            'pattern_types_distribution': pattern_types,
            'most_common_patterns': most_common_patterns,
            'high_confidence_patterns': [p.to_dict() for p in high_confidence_patterns],
            'patterns_summary': [p.to_dict() for p in patterns_list],
            'generated_at': datetime.now(),
            'ml_available': ML_AVAILABLE
        }


# ===================================================================
# INTERFACE STREAMLIT
# ===================================================================

def display_pattern_recognition_interface():
    """Interface Streamlit pour Pattern Recognition SafetyGraph"""
    import streamlit as st
    import pandas as pd
    import numpy as np
    import plotly.graph_objects as go
    import plotly.express as px
    from datetime import datetime, timedelta
    
    st.markdown("# ğŸ” Pattern Recognition SafetyGraph")
    st.markdown("---")
    
    # Header avec mÃ©triques
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ğŸ¯ Patterns IdentifiÃ©s", "15", "+3")
    
    with col2:
        st.metric("ğŸ” Clusters Actifs", "4", "+1")
    
    with col3:
        st.metric("ğŸ“Š PrÃ©cision Clustering", "87.3%", "+2.1%")
    
    with col4:
        st.metric("ğŸ”„ DerniÃ¨re Analyse", "5min", "")
    
    st.markdown("---")
    
    # Configuration clustering
    st.markdown("## âš™ï¸ Configuration Clustering")
    
    col1, col2 = st.columns(2)
    
    with col1:
        clustering_method = st.selectbox(
            "ğŸ§  MÃ©thode Clustering",
            ["K-Means", "DBSCAN", "Hierarchical", "Gaussian Mixture"],
            index=0
        )
        
        n_clusters = st.slider(
            "ğŸ¯ Nombre de Clusters",
            min_value=2,
            max_value=10,
            value=4,
            step=1
        )
    
    with col2:
        features_selection = st.multiselect(
            "ğŸ“Š Features Ã  Analyser",
            ["Leadership", "Communication", "Formation", "Participation", "Suivi", "Environnement"],
            default=["Leadership", "Communication", "Formation"]
        )
        
        min_cluster_size = st.slider(
            "ğŸ“ Taille Min Cluster",
            min_value=5,
            max_value=50,
            value=10,
            step=5
        )
    
    if st.button("ğŸš€ Lancer Analyse Patterns", use_container_width=True):
        with st.spinner("ğŸ”„ Analyse des patterns en cours..."):
            import time
            time.sleep(2)
            
            # Simulation donnÃ©es patterns
            patterns_data = []
            
            # GÃ©nÃ©ration patterns simulÃ©s
            pattern_types = ["Proactif", "RÃ©actif", "Ã‰mergent", "Mature"]
            sectors = ["Construction", "Manufacturing", "Healthcare", "Transportation"]
            
            for i in range(n_clusters):
                pattern = {
                    'Cluster': f'Cluster {i+1}',
                    'Type': np.random.choice(pattern_types),
                    'Secteur_Dominant': np.random.choice(sectors),
                    'Taille': np.random.randint(15, 80),
                    'Score_Moyen': np.random.uniform(2.5, 4.5),
                    'StabilitÃ©': np.random.uniform(0.6, 0.95),
                    'Tendance': np.random.choice(['AmÃ©lioration', 'Stable', 'DÃ©clin'])
                }
                patterns_data.append(pattern)
            
            patterns_df = pd.DataFrame(patterns_data)
            
            st.success("âœ… Analyse patterns terminÃ©e!")
            
            # RÃ©sultats clustering
            st.markdown("## ğŸ“Š RÃ©sultats Clustering")
            
            # Tableau des clusters
            st.dataframe(patterns_df, use_container_width=True)
            
            # Visualisation clusters
            st.markdown("### ğŸ¯ Visualisation Clusters")
            
            # Graphique scatter des clusters
            fig_scatter = px.scatter(
                patterns_df,
                x='Score_Moyen',
                y='StabilitÃ©',
                size='Taille',
                color='Type',
                hover_data=['Cluster', 'Secteur_Dominant', 'Tendance'],
                title="ğŸ” Distribution des Clusters"
            )
            fig_scatter.update_layout(template="plotly_dark")
            st.plotly_chart(fig_scatter, use_container_width=True)
            
            # Graphique en barres des types
            fig_bar = px.bar(
                patterns_df,
                x='Type',
                y='Taille',
                color='Tendance',
                title="ğŸ“ˆ RÃ©partition Types de Patterns"
            )
            fig_bar.update_layout(template="plotly_dark")
            st.plotly_chart(fig_bar, use_container_width=True)
            
            # Analyse dÃ©taillÃ©e par cluster
            st.markdown("## ğŸ” Analyse DÃ©taillÃ©e par Cluster")
            
            selected_cluster = st.selectbox(
                "SÃ©lectionner un cluster",
                patterns_df['Cluster'].tolist()
            )
            
            cluster_data = patterns_df[patterns_df['Cluster'] == selected_cluster].iloc[0]
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("### ğŸ“Š CaractÃ©ristiques")
                st.info(f"**Type:** {cluster_data['Type']}")
                st.info(f"**Secteur Dominant:** {cluster_data['Secteur_Dominant']}")
                st.info(f"**Taille:** {cluster_data['Taille']} entreprises")
                st.info(f"**Score Moyen:** {cluster_data['Score_Moyen']:.2f}/5.0")
            
            with col2:
                st.markdown("### ğŸ¯ MÃ©triques")
                st.metric("StabilitÃ©", f"{cluster_data['StabilitÃ©']:.2%}")
                st.metric("Tendance", cluster_data['Tendance'])
                
                # Recommandations
                st.markdown("### ğŸ’¡ Recommandations")
                if cluster_data['Score_Moyen'] < 3.0:
                    st.error("ğŸš¨ Action immÃ©diate requise")
                    st.info("â€¢ Audit approfondi culture sÃ©curitÃ©")
                    st.info("â€¢ Formation leadership intensifiÃ©e")
                elif cluster_data['Score_Moyen'] < 4.0:
                    st.warning("âš ï¸ AmÃ©lioration recommandÃ©e")
                    st.info("â€¢ Renforcement communication")
                    st.info("â€¢ Suivi rÃ©gulier indicateurs")
                else:
                    st.success("âœ… Cluster performant")
                    st.info("â€¢ Maintenir les bonnes pratiques")
                    st.info("â€¢ Partager les success stories")
    
    # Section patterns temporels
    st.markdown("---")
    st.markdown("## â±ï¸ Patterns Temporels")
    
    # GÃ©nÃ©ration donnÃ©es temporelles
    dates = pd.date_range(end=datetime.now(), periods=12, freq='M')
    
    temporal_data = pd.DataFrame({
        'Date': dates,
        'Proactif': np.random.normal(30, 5, 12),
        'RÃ©actif': np.random.normal(25, 4, 12),
        'Ã‰mergent': np.random.normal(20, 3, 12),
        'Mature': np.random.normal(25, 4, 12)
    })
    
    fig_temporal = go.Figure()
    
    for pattern_type in ['Proactif', 'RÃ©actif', 'Ã‰mergent', 'Mature']:
        fig_temporal.add_trace(go.Scatter(
            x=temporal_data['Date'],
            y=temporal_data[pattern_type],
            mode='lines+markers',
            name=pattern_type
        ))
    
    fig_temporal.update_layout(
        title="ğŸ“ˆ Ã‰volution Patterns Temporels",
        xaxis_title="Date",
        yaxis_title="Nombre d'Entreprises",
        template="plotly_dark"
    )
    
    st.plotly_chart(fig_temporal, use_container_width=True)
    
    # Section corrÃ©lations
    st.markdown("---")
    st.markdown("## ğŸ”— Analyse CorrÃ©lations")
    
    # Matrice de corrÃ©lation simulÃ©e
    correlation_data = np.random.rand(6, 6)
    correlation_data = (correlation_data + correlation_data.T) / 2
    np.fill_diagonal(correlation_data, 1.0)
    
    dimensions = ["Leadership", "Communication", "Formation", "Participation", "Suivi", "Environnement"]
    correlation_df = pd.DataFrame(correlation_data, index=dimensions, columns=dimensions)
    
    fig_corr = px.imshow(
        correlation_df,
        title="ğŸ”— Matrice CorrÃ©lations Dimensions",
        color_continuous_scale="RdBu",
        aspect="auto"
    )
    fig_corr.update_layout(template="plotly_dark")
    st.plotly_chart(fig_corr, use_container_width=True)
    
    # Section transitions patterns
    st.markdown("---")
    st.markdown("## ğŸ”„ Transitions Patterns")
    
    # Simulation donnÃ©es transitions
    transitions_data = {
        'De': ['Ã‰mergent', 'Ã‰mergent', 'RÃ©actif', 'RÃ©actif', 'Proactif', 'Mature'],
        'Vers': ['RÃ©actif', 'Proactif', 'Proactif', 'Mature', 'Mature', 'Proactif'],
        'ProbabilitÃ©': [0.7, 0.3, 0.6, 0.2, 0.4, 0.1],
        'FrÃ©quence': [45, 18, 32, 12, 25, 8]
    }
    
    transitions_df = pd.DataFrame(transitions_data)
    
    # Graphique Sankey pour transitions
    fig_sankey = go.Figure(data=[go.Sankey(
        node = dict(
            pad = 15,
            thickness = 20,
            line = dict(color = "black", width = 0.5),
            label = ["Ã‰mergent", "RÃ©actif", "Proactif", "Mature"],
            color = ["#FF6B6B", "#FFA07A", "#98D8C8", "#6BCF7F"]
        ),
        link = dict(
            source = [0, 0, 1, 1, 2, 3],
            target = [1, 2, 2, 3, 3, 2],
            value = [45, 18, 32, 12, 25, 8]
        )
    )])
    
    fig_sankey.update_layout(
        title_text="ğŸ”„ Flux Transitions entre Patterns",
        template="plotly_dark"
    )
    st.plotly_chart(fig_sankey, use_container_width=True)
    
    # Tableau dÃ©taillÃ© des transitions
    st.markdown("### ğŸ“‹ DÃ©tail des Transitions")
    
    # Formatage du tableau
    transitions_display = transitions_df.copy()
    transitions_display['ProbabilitÃ©'] = transitions_display['ProbabilitÃ©'].apply(lambda x: f"{x:.1%}")
    transitions_display.columns = ['ğŸ”„ De', 'â¡ï¸ Vers', 'ğŸ“Š ProbabilitÃ©', 'ğŸ”¢ FrÃ©quence']
    
    st.dataframe(transitions_display, use_container_width=True)
    
    # Section benchmarking
    st.markdown("---")
    st.markdown("## ğŸ“Š Benchmarking Sectoriel")
    
    # DonnÃ©es benchmarking simulÃ©es
    benchmark_data = {
        'Secteur': ['Construction', 'Manufacturing', 'Healthcare', 'Transportation', 'Services'],
        'Pattern_Dominant': ['RÃ©actif', 'Proactif', 'Mature', 'Ã‰mergent', 'Proactif'],
        'Score_Moyen': [3.2, 4.1, 4.3, 2.8, 3.9],
        'MaturitÃ©': ['DÃ©veloppement', 'AvancÃ©', 'Excellence', 'Ã‰mergent', 'AvancÃ©'],
        'Rang': [4, 2, 1, 5, 3]
    }
    
    benchmark_df = pd.DataFrame(benchmark_data)
    
    # Graphique radar benchmarking
    fig_radar = go.Figure()
    
    fig_radar.add_trace(go.Scatterpolar(
        r=benchmark_df['Score_Moyen'],
        theta=benchmark_df['Secteur'],
        fill='toself',
        name='Scores Sectoriels',
        line_color='#4ECDC4'
    ))
    
    fig_radar.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 5]
            )),
        title="ğŸ“Š Benchmarking Scores par Secteur",
        template="plotly_dark"
    )
    
    st.plotly_chart(fig_radar, use_container_width=True)
    
    # Tableau benchmarking
    st.markdown("### ğŸ† Classement Sectoriel")
    
    benchmark_display = benchmark_df.copy()
    benchmark_display = benchmark_display.sort_values('Rang')
    benchmark_display.columns = ['ğŸ­ Secteur', 'ğŸ¯ Pattern Dominant', 'ğŸ“Š Score Moyen', 'ğŸ“ˆ Niveau MaturitÃ©', 'ğŸ† Rang']
    
    st.dataframe(benchmark_display, use_container_width=True)
    
    # Insights automatiques
    st.markdown("---")
    st.markdown("## ğŸ§  Insights Automatiques")
    
    insights = [
        "ğŸ¯ Le pattern 'Proactif' montre une forte corrÃ©lation avec le leadership (r=0.87)",
        "ğŸ“Š Les secteurs Construction et Manufacturing partagent des patterns similaires",
        "âš¡ Ã‰mergence d'un nouveau pattern 'Hybride' dans le secteur Healthcare",
        "ğŸ”„ Cycles saisonniers dÃ©tectÃ©s dans les patterns de formation",
        "ğŸš¨ Alertes: 3 clusters montrent des signes de dÃ©gradation",
        "ğŸ“ˆ Tendance gÃ©nÃ©rale: transition vers patterns plus matures (+15% en 6 mois)",
        "ğŸ¯ OpportunitÃ©: secteurs Ã©mergents prÃªts pour Ã©volution pattern"
    ]
    
    for insight in insights:
        st.info(insight)
    
    # Section export et actions
    st.markdown("---")
    st.markdown("## ğŸ“¤ Export et Actions")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### ğŸ“Š Export DonnÃ©es")
        
        if st.button("ğŸ“¥ TÃ©lÃ©charger Rapport PDF", use_container_width=True):
            st.info("ğŸ“„ Rapport PDF gÃ©nÃ©rÃ© et tÃ©lÃ©chargÃ©!")
        
        if st.button("ğŸ“Š Export Excel DÃ©taillÃ©", use_container_width=True):
            st.info("ğŸ“Š DonnÃ©es exportÃ©es vers Excel!")
        
        if st.button("ğŸ”— Partager Dashboard", use_container_width=True):
            st.info("ğŸ”— Lien de partage gÃ©nÃ©rÃ©!")
    
    with col2:
        st.markdown("### ğŸ¯ Actions RecommandÃ©es")
        
        actions = [
            "ğŸ” Approfondir analyse Cluster 2",
            "ğŸ“Š Surveiller transition Ã‰mergentâ†’RÃ©actif",
            "ğŸ¯ Benchmarker avec secteur Healthcare",
            "âš¡ DÃ©ployer pattern Proactif en Construction",
            "ğŸš¨ Audit urgent clusters en dÃ©clin"
        ]
        
        for action in actions:
            if st.button(action, use_container_width=True):
                st.success(f"âœ… Action planifiÃ©e: {action}")
    
    # Section configuration avancÃ©e
    st.markdown("---")
    st.markdown("## ğŸ”§ Configuration AvancÃ©e")
    
    with st.expander("âš™ï¸ ParamÃ¨tres Algorithmes"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Clustering:**")
            epsilon = st.slider("DBSCAN Epsilon", 0.1, 2.0, 0.5, 0.1)
            min_samples = st.slider("DBSCAN Min Samples", 2, 20, 5)
            
        with col2:
            st.markdown("**Pattern Recognition:**")
            confidence_threshold = st.slider("Seuil Confiance", 0.5, 1.0, 0.7, 0.05)
            stability_window = st.slider("FenÃªtre StabilitÃ© (jours)", 7, 90, 30)
    
    with st.expander("ğŸ”” Alertes et Notifications"):
        st.markdown("**Configuration Alertes:**")
        
        alert_new_pattern = st.checkbox("Nouveau pattern dÃ©tectÃ©", value=True)
        alert_transition = st.checkbox("Transition pattern critique", value=True)
        alert_degradation = st.checkbox("DÃ©gradation cluster", value=True)
        
        notification_email = st.text_input("Email notifications", "admin@safetygraph.com")
        notification_frequency = st.selectbox("FrÃ©quence", ["Temps rÃ©el", "Quotidien", "Hebdomadaire"])
    
    # Footer
    st.markdown("---")
    st.markdown("**ğŸ” Pattern Recognition SafetyGraph** | Powered by ML & Advanced Analytics")
    st.markdown("*DerniÃ¨re mise Ã  jour: " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "*")